{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de7c4b5-a5ca-4fb5-9887-e6e8dd3ce61f",
   "metadata": {},
   "source": [
    "<h1> Gradient Descent - A Detailed Discussion</h1>\n",
    "Gradient descent is the most popular optimization algorithm. When the evaluation cost of the objective is high, or there are many optimization parameters, GC is the only possible approach. Also, there are good implementations of GC for paralell computing. In this chapter, we review Tensorflow optimizer and we study different flavors of GC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd08ca-bed1-45ea-bae9-4166a4e1acc2",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gstatic.com/devrel-devsite/prod/vee468f4e10aa470182a016132769d1277f3b792f56b19f433715afc734e9c71d/tensorflow/images/lockup.svg\" width=200>  <img src=\"https://keras.io/img/logo.png\" width=150><br>\n",
    "Tensorflow is an open source free library for machine learning. Tensorflow is donated to public by Google and has a large body of developers. Tensorflow is one major tools for training and evaluation of deep learning models. It also has an optimizer which is used for training DL models. Keras is another open source project which acts as an interface for tensorflow. Keras makes using Tensorflow easier. Also, keras supports other platforms too. Tensorflow has an internal module for keras. However, keras itself can be installed separately.  <br>\n",
    "For installing tensorflow using the following command: ```conda install -c conda-forge tensorflow```<br>\n",
    "For Keras use this command: ```conda install -c conda-forge keras```<br><br>\n",
    "In this chapter we focus on ```tensorflow.keras.optimizaers```.<br>\n",
    "Let's have a refresher first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0bd3b3-6a16-40f6-9d23-829465b9ed4b",
   "metadata": {},
   "source": [
    "<h2> A Tensorflow Refresher</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c041ea0-882c-43a9-a4c4-d8e029dd2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84eb37-bed3-4925-9710-97f74d656fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1., 2.],\n",
    "                 [3., 4.]])\n",
    "print(\"x: \", x)\n",
    "print(\"shape: \", x.shape)\n",
    "print(\"dtype: \", x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4b0d9-41ad-4fa2-a597-2ee63bd5efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to numpy\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4525b-8b8c-4c12-aa80-f2bd99b37f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d5c26-87d9-4977-ab99-e94410d98432",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236e99f-196e-4ba5-ae7a-c75c30019ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4f856-3eb6-447e-a90a-ee77f9b3b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor add\n",
    "x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7b3fe-1c4d-4d2a-9bec-7ba2a9835e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar tensor multiply\n",
    "6 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee922f62-cd65-48fb-847b-f88b05a1b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor transpose\n",
    "tf.transpose(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdce1eb-05fe-4f1b-a71d-ea80e9635d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix(tensor) multiplication\n",
    "x @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf26668-3753-4c7e-b459-5fc10c696d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor concat\n",
    "tf.concat([x, x, x], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8facf-d744-4e86-ab16-5bf68304af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tf.sin(np.pi/2), tf.cos(np.pi/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83021087-699c-484b-8aab-632641c1217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497872c5-1cd3-4f50-ae63-7c815c0a64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e893c-e6de-4b80-8617-a1c129b15d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d95121-a99c-42d1-b206-5f87d9a1822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape a tensor\n",
    "x = tf.constant([[1., 2.], [3., 4.]])\n",
    "x2= tf.reshape(x, shape=(1,4))\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43745927-fa00-4272-89c8-d8b021ce12d6",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=70><font size=4>Ragged Tensors:</font><br>\n",
    "A tensor with variable numbers of elements along some axis is called \"ragged\".<br>\n",
    "<img src=\"https://www.tensorflow.org/guide/images/tensor/ragged.png\" alt=\"A 2-axis ragged tensor, each row can have a different length.\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efef3f-1e34-4c3d-81c3-38766d4724a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ragged_list = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a346700-bfef-4b70-b672-0e3285d94a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line raise an exception sicne tensor has to be rectangular\n",
    "tensor = tf.constant(ragged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a3c95-c5f4-4613-9643-c9fa1c166a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead use the ragged tensor\n",
    "ragged_tensor = tf.ragged.constant(ragged_list)\n",
    "ragged_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6355ef-5b0e-43ab-a2df-240f71c9c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_of_strings = tf.constant([\"Alice\", \"Bob\",\"Mark\"])\n",
    "tensor_of_strings # It prints out elements with a b prefix pointing out that strings are not unicode rather byte string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a9e21-72c9-4f93-b19b-8ec3608302a0",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=70><font size=4>Sparse Tensors:</font><br>\n",
    "If your tensor is sparse (having many zero elements) it is a good idea to store it as a sparse representation rather than dense. <br>\n",
    "<img src=\"https://www.tensorflow.org/guide/images/tensor/sparse.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966ddb3-4c42-42b8-9164-ce8e948b4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],\n",
    "                                       values=[1, 2],\n",
    "                                       dense_shape=[3, 4])\n",
    "sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047dad30-1385-4cbe-a67f-79f3c812608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sparse.to_dense(sparse_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51125990-8043-44e1-99d8-4bc65c420afe",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-itim2101-lineal-color-itim2101/344/external-professor-life-style-avatar-itim2101-lineal-color-itim2101.png\" alt=\"Instructor\" width=50><font size=4>Variables:</font>Tensorflow has variables. Variables are wrapper around tensors. Variables are mutable but tensors are immutable. More over variables operates with tensorflow in some specific computations needed for learning a model. At the end a variable can be a piece of memory accessed by CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae02b83-36cb-4e2a-af65-046a19109c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable([[0.0, 0.0, 0.0], [1., 2., 3.]])\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442f457-cda4-44e8-b814-b8d9db871570",
   "metadata": {},
   "outputs": [],
   "source": [
    "var.assign([[2., 3., 4.], [5., 6., 7.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d37b48-2009-4a2d-ae78-7988cd0f2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "var.assign_add([[1., 1., 1.], [2., -1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b7938-89d9-45fa-ab3b-c17993587ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will raise an exception. The shape has to be the same\n",
    "var.assign([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514734d-db90-4081-ace7-ba73b43d4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x**2 + 2*x - 5\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1989c-28b8-4907-869d-71cf7c206768",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d2d99-36cc-4aa6-aaee-5d79691373dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a new variable. \n",
    "var2= tf.reshape(var, shape=[1,6])\n",
    "var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f65ac2-d07b-4b62-b6be-be8f230d94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify a name for variables. They are used in serialization and restoring object. Names are uniques inside the scope.\n",
    "x = tf.Variable([1., 2., 3.], name=\"MyVariable\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c01d58-a0ac-44cd-b42c-d9a7b0e38724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, there is a trainable argument inside the variable init.\n",
    "x = tf.Variable([1., 2.], trainable=False)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566c575-09cd-4d72-8a2f-243b5eaef74f",
   "metadata": {},
   "source": [
    "<html>\n",
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=70><font size=4>Placing on specific Backend:</font><br>\n",
    "Tensorflow tries to put the variable on the most efficient available backend. If you have a platform with multiple backend you can override rules and place a variable on specific backend like:\n",
    "<code>\n",
    "with tf.device('CPU:0'):\n",
    "    a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "print(c)\n",
    "</code></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7776a-ce89-453f-9e0c-35f254f4e884",
   "metadata": {},
   "source": [
    "<h2>Graident Tape: </h2>  You can use tensorflow (Jax under hood) to calculate the gradients. For doing that, you need to start a tape to record all activities (__watch__ them)then asking tensorflow to calculate the gradient using the recording on the tape by calling ```GradientTape.gradient(target, sources)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782c0bb-933a-41e4-8583-414b7fb80387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vars are jax compatible. You can calculate the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x)  # g(x) = dy/dx\n",
    "g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9a661-11df-4db5-9370-ab0224d3ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainable variable and non-trainable\n",
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2', trainable=True) + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**3) + (x2**4) + (x3**5)\n",
    "\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83965b0-f0ab-4b97-9753-461a791912a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all watched values in the tape\n",
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad87db1-66de-4eca-8125-3907d76925f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can watch stuffs manually\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable but can be watched\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# A tensor can be watched\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# A constant is not going to be watched\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x0)\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    y = (x0**2) + (x1**3) + (x2**4)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edb9d6-8c01-432a-9553-afb4b1f7a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run this example one more time here\n",
    "x = tf.Variable(4.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y =  x**3\n",
    "    z = x**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b0e66-6264-47c6-ac96-388283f1ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line runs fine\n",
    "tape.gradient(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003997cc-f27b-4ae9-b3c3-38798f0b0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you run this command you will get an error\n",
    "tape.gradient(z,x)  # When you define the tape as context manager, after you call gradient, it frees the tap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2940b-6af0-441e-94be-138af6bf3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(4.)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x**3\n",
    "    z = x**4\n",
    "print(tape.gradient(y,x))\n",
    "print(tape.gradient(z,x))\n",
    "del tape # Free the resource manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d946016-1d20-42a8-bfff-da151760aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above we can nest gradient caluclations / This will raise a performance warning.\n",
    "x = tf.Variable(1.)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x**3\n",
    "    z = tape.gradient(y,x)\n",
    "w = tape.gradient(z, x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac96720-e3da-485e-9d69-c73b8cb3d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to nest two tapes\n",
    "# using the above we can nest gradient caluclations\n",
    "x = tf.Variable(1.)\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "        y = x**3\n",
    "    z = inner_tape.gradient(y,x)\n",
    "w = outer_tape.gradient(z, x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682eab0-2859-4956-898f-1a060fee31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can temporary stop recording\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    x_sq = x * x\n",
    "    with t.stop_recording():\n",
    "        y_sq = y * y\n",
    "    z = x_sq + y_sq\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y}) # You can pass a named -value dict instead of a list.\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313c883-5605-4743-b9e6-55006b4a890c",
   "metadata": {},
   "source": [
    "<img src= \"https://img.icons8.com/external-flaticons-flat-flat-icons/344/external-question-100-most-used-icons-flaticons-flat-flat-icons.png\" alt=\"Tip\" width=70>  For the function f(x, y) = sin((x+y)^2) + cos(x*y/2), and desired points of {(x,y)} = {(pi/2, pi/3), (pi/4,pi/5), (pi/6,pi/7)}, Write a program to do the following:<br>\n",
    "<ul>\n",
    "    <li> Evaluate the function at desried point</li>\n",
    "    <li> What is the value of the first, second and third derivative of function at desired point</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36a45d-0196-4709-a93e-e683ef1e0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(1.0, 10.0, 11)\n",
    "delta = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.nn.sigmoid(x+delta)\n",
    "\n",
    "dy_dx = tape.jacobian(y, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54fe46-caae-43d6-97e3-3d38818bdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a01372-8b03-458c-9f08-efbee0599d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control flow and gradient\n",
    "x = tf.constant(1.0)\n",
    "y = tf.Variable(2.0)\n",
    "z = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    if x > 0.0:\n",
    "        w = y\n",
    "    else:\n",
    "        w = z**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(w, [y, z])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1) # this will return None since based on the condition this is not connected to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3e2ce-25fb-4132-977d-b9cc55f2fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of multiple function (not using jacobian)\n",
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "\n",
    "y = [y0.numpy(), y1.numpy()]\n",
    "g_y = [tape.gradient(y0, x).numpy(), tape.gradient(y1, x).numpy()]\n",
    "print(y)\n",
    "print(g_y)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05327304-72b4-496b-b5d1-7f1a956421b6",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-itim2101-lineal-color-itim2101/344/external-professor-life-style-avatar-itim2101-lineal-color-itim2101.png\" alt=\"Instructor\" width=50> <font size=4> Gradient of Non-Scalar Targets:</font><br>\n",
    "A gradient is fundamentally an operation on a scalar. If you force tensorflow to return the gradient of a non-scalar target, they will be added together. This is chosen by desing to make calculation of gradient of sum of element (like sum of error in linear regression). If you need gradient of a vector function, use jacobian instead. Let's demonstrate it using examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca303b58-0632-4bee-b89d-c47555ad85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient on non-scalar target example 1\n",
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52be5a-a8d2-4a75-80ea-d881f9465713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient on non-scalar target example 2\n",
    "x = tf.Variable(2.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * [3., 4.]\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17bdd6-4046-417c-96a8-d726c914717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "var1 = tf.Variable(10.0)\n",
    "loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "step_count = opt.minimize(loss, [var1]).numpy()\n",
    "# The first step is `-learning_rate*sign(grad)`\n",
    "var1.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2be62d-f8c9-4c97-aa4e-a2750b281581",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-itim2101-lineal-color-itim2101/344/external-professor-life-style-avatar-itim2101-lineal-color-itim2101.png\" alt=\"Instructor\" width=50> <font size=4> Gradient of Non-Tnesorflow Calculations:</font><br>\n",
    "As you may guess, Gradient Tape is not able to caluclate the gradient of non-tensorflow operations. Also, TF can't calculate the gradient for int or string. Let's demonstrate it using an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588afdb-420e-4106-be1a-36bb9ca7cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    x2 = x**2\n",
    "\n",
    "    # This step is calculated with NumPy\n",
    "    y = np.mean(x2, axis=0)\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y = tf.reduce_mean(y, axis=0)\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ced33-ae44-45ee-ac61-faae1618605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is not possible to calculate the gradient of int.\n",
    "x = tf.constant(10)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970839ec-b771-493a-8398-00831aabcf71",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-itim2101-lineal-color-itim2101/344/external-professor-life-style-avatar-itim2101-lineal-color-itim2101.png\" alt=\"Instructor\" width=50> <font size=4> Gradient of Stateful Object:</font><br>\n",
    "By default, Tensorflow can calculate the gradient of stateful object. When the tape gets to a stateful object, it stops there. Let's have an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03e059-85a0-455a-99cc-de2432620dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Update x1 = x1 + x0.\n",
    "    x1.assign_add(x0)\n",
    "    # The tape starts recording from x1.\n",
    "    y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23a901-b4da-4f62-a594-ee1b6b992e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Prep\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = tf.constant(3.)\n",
    "b = tf.constant(2.)\n",
    "noise_std = tf.constant(0.1)\n",
    "x = tf.random.uniform(shape=(1,100))\n",
    "y = a * x + b + noise_std * tf.random.normal(shape=(1,100))\n",
    "\n",
    "_=plt.scatter(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d268d-eb2e-4808-a5b6-6a8a549972c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a and b using SSE and optimization\n",
    "eta = 0.001\n",
    "a_hat = 1.\n",
    "b_hat = 1.\n",
    "for i in range(400):\n",
    "    v_a = tf.Variable(a_hat, dtype=tf.float32)\n",
    "    v_b = tf.Variable(b_hat, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_sum(((v_a * x + v_b) - y)**2.)\n",
    "    g_a, g_b = tape.gradient(loss, [v_a, v_b])\n",
    "    a_hat = a_hat - eta * g_a.numpy()\n",
    "    b_hat = b_hat - eta * g_b.numpy()\n",
    "\n",
    "a_hat, b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5931e6-7ca8-4e39-b29f-337fe7319a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "x_plt = np.array([x.numpy().min(), x.numpy().max()])\n",
    "plt.plot(x_plt, a_hat * x_plt + b, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5a325-58fe-4f89-991b-e958d884e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way is using Keras which makes it easier\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "a_hat = tf.Variable(1.)\n",
    "b_hat = tf.Variable(1.)\n",
    "theta = [a_hat, b_hat]\n",
    "eta = tf.constant(0.1)\n",
    "for i in range(400):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = ((a_hat * x + b_hat) - y)**2.  # Reduce sume is omitted on purpose.\n",
    "    g_theta = tape.gradient(loss, theta)\n",
    "    opt.apply_gradients(zip(g_theta, theta)) # Remember: we can't directly \n",
    "a_hat, b_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfa16f-8afa-440e-bff4-5e16406b73b2",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=50> __Tip:__ <br> <code>tf.keras.optimizers.SGD</code> is one of Keras optimizers. We will discuss it in details later. Here we used <code>apply_gradients</code>, it also has <code>minimize</code> function which is much easier and you don't need to work with the tape. However, <code>apply_gradients</code> is used if you wish to do some post proecessing on gradients before using them. This is very useful for clipping gradients. We will come back to clipping later in an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900fd56-780f-4fc9-b5f6-78ff309bf43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1> Gradient Descent Using Keras</h1>\n",
    "In chapter 2, we reviewed vanilla gradient descent as below:\n",
    "<img src=\"images/vanilla_gd.png\" width=250> <br>\n",
    "In many optimization problems, the objective function f has the following form:<br>\n",
    "<img src=\"images/sum_err.png\" width=450> \n",
    "An example of this form of objective function is SSE error in a linear regression:<br>\n",
    "<img src=\"images/sum_err_ex.png\" width=300>\n",
    "If you want to calculate the gradient you should iterate over N (for example number of training samples). This might take a while to go through and at the end you just make one step update. This is called full batch gradient decent. Another alternative is update the optimal point using just one sample (estiating the gradient), this is called stochastic gradient decent. This is good since for one update we don't have to go through the whole dataset. However, using one sample can be very noisy. specifically, when we are near the optimal point and we need precision. A good comporomize is using a sub sample of the dataset called mini-batch (let's say 50 samples) and calculating gradient using mini-batch. This is better in terms of accuracy of estimate and computation performance.  This method is called stochastic mini-batch gradient descent. Here we use mini-batch or stochastic GD interchangably and we refer to this method with it.\n",
    "<img src=\"images/mini_batch.png\" width=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cee65b-1fc4-4845-98a2-14077226ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regardless of your objective type you can use the following command to implement SGD\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "x = tf.Variable(1.0)\n",
    "y = tf.Variable(2.0)\n",
    "loss = lambda: (x**2 + y**2)/2.0     \n",
    "for i in range(100):\n",
    "    step_count = opt.minimize(loss, [x,y]).numpy()\n",
    "# Step is `- learning_rate * grad`\n",
    "x.numpy(), y.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f5d85-d2ce-4800-a65a-0e2ce3a072fd",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-soft-fill-juicy-fish/344/external-maths-school-soft-fill-soft-fill-juicy-fish.png\" alt=\"Math Tip\" width=50> <font size=5> Momentum: </font>\n",
    "If the objective function is very steep in one dimension but not much in other, GD jumps around and it slows the covergence. <br>\n",
    "<img src=\"images/momentum.png\" width =400><br>\n",
    "This limits the choice of learning rate. If you choose a big learning rate, the problem can diverge. Overall in this situation tuning learning rate becomes very hard. A solution for fighting this situation is adding a term to GD called momentum.<br>\n",
    "<img src=\"images/momentum_eq.png\" width=300><br>\n",
    "You can introduce momentum to (all implementation) of GD in tensorflow. You just need to set the gamma parameter. Gamma itself is a hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddd470-8ee1-4e9c-84c8-32208d9d0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.1)\n",
    "x = tf.Variable(1.0)\n",
    "y = tf.Variable(2.0)   \n",
    "for i in range(100):\n",
    "    loss = lambda: (x**2 + y**2)/2.0  \n",
    "    step_count = opt.minimize(loss, [x,y]).numpy()\n",
    "# Step is `- learning_rate * grad`\n",
    "x.numpy(), y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c401181-0a4a-4c55-b4a0-f4dc1f525c45",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-soft-fill-juicy-fish/344/external-maths-school-soft-fill-soft-fill-juicy-fish.png\" alt=\"Math Tip\" width=50> <font size=5> Nesterov Accelerated Gradient: </font>\n",
    "In order to improve momentum, Nesterov Accelerated Gradient (NAG), use the value of gradient at look ahead point instead of the current point. This can increase the speed of convergence. <br>\n",
    "<table><tr><td><img src=\"images/nag_eq.png\" width=300></td><td><img src=\"images/nag_concept.png\" width=600></td></tr></table><br>\n",
    "Check the following illustrative example:<br><br>\n",
    "<img src=\"images/nag_opt.png\" width=600><br>\n",
    "In tensorflow for (all GD implementations) set <code>nesterov = True</code> for using Nesterov momentum update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7db7-686a-410c-9cd2-8cbb24743807",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.1, nesterov=True)\n",
    "x = tf.Variable(1.0)\n",
    "y = tf.Variable(1.0)     \n",
    "for i in range(100):\n",
    "    loss = lambda: (x**2 + y**2)/2.0\n",
    "    step_count = opt.minimize(loss, [x,y]).numpy()\n",
    "# Step is `- learning_rate * grad`\n",
    "x.numpy(), y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b2af8-791f-4788-a345-6e96f780d260",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://img.icons8.com/external-kosonicon-lineal-color-kosonicon/344/external-lab-tool-back-to-school-kosonicon-lineal-color-kosonicon.png\" alt=\"Lab\" width=80 > __Polynomial Model:__ We are interested in fitting a model like ax^2 + bx + c. Let's assume a = -1 , b = 2,  c = 3. Generate training samples (Gaussian noise with standard dev of 0.04). Then try to fit the model using SSE criteria. In optimization use SGD with momentum and Nesterov.\n",
    "\n",
    "<b>Hint:</b> Start with a small number of iterations (epochs) and try to adjust the learning rate. Then, iterate for a good number of iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87313a9-d78c-4f07-b676-5741f63a7d40",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://img.icons8.com/external-soft-fill-juicy-fish/344/external-maths-school-soft-fill-soft-fill-juicy-fish.png\" alt=\"Math Tip\" width=50> <font size=5> Adaptive Learning Rate: </font><br>\n",
    "As you might have seen by above lab, tuning learning rate is important. If you choose large learning rate, it is fast but it can diverge, if you choose small learning rate, it won't diverge but it will take too long to converge. A good solution here, is starting with a large learning rate and then as we get close to the optimal point taking smaller steps. This is called adaptive learning rate. Here, we review two famous algorithm for adaptive learning rate:<br>\n",
    "<h2>Adagrad</h2>\n",
    "Adaptive gradient algorithm (AdaGrad) adapts learning rate separately for different parameters. It perform larger updates for infrequent and smaller updates for frequent parameters which makes it suitable for dealing with sparse datasets. (It is originall developed by Jeff Dean from Google).\n",
    "<img src=\"images/adagrad.png\" width=250><br>\n",
    "You can use <code>tf.keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, epsilon)</code> to define an adagrad GD.\n",
    "<h2>Adadelta</h2>\n",
    "Adadelta is an extension to Adagrad. It restricts the window of accumulated past gradient to a fixed size like $w$. Instead of inefficiently storing all past gradients , a decaying average is used. \n",
    "<table><tr><td><img src=\"images/adadelta_update.png\" width=300></td><td> Where:</td><td> <img src=\"images/adadelta_adaptive.png\" width=300></td></tr></table>\n",
    "You can use <code>tf.keras.optimizers.Adadelta(learning_rate,rho, epsilon),</code> to define an adagrad GD. Where rho is the beta in above equations.<br>\n",
    "\n",
    "In using these functions, if you leave parameters, tensorflow will \n",
    "fill the with default values suggested in research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bf9f4-3783-4991-8a26-2b085e0357f9",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-kosonicon-lineal-color-kosonicon/344/external-lab-tool-back-to-school-kosonicon-lineal-color-kosonicon.png\" alt=\"Lab\" width=50 > <font size=5>Paper Box Problem:</font>\n",
    "Given a square piece of paper with side length of L = 30. Find the cutting place x which the maximize the volume of box.\n",
    "<img src=\"http://jwilson.coe.uga.edu/EMT725/Box/Image20.jpg\" width=500><br>\n",
    "After writing the problem solve it using Adagrad and Adadelta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb583ff2-ef8d-4a07-a7db-65cd47ad3289",
   "metadata": {},
   "source": [
    "<h2> RMSProp</h2>\n",
    "RMSProp is similar to Adagrad, it just uses a leaky sum to adjust learning rates.<br>\n",
    "<img src=\"images/rmsprop.png\" width=300><br>\n",
    "In tensorflow you can use RMSProp as <code>tf.keras.optimizers.RMSprop(learning_rate, rho,momentum)</code>\n",
    "The following animation compares the so far discussed method for an objective function.\n",
    "<img src=\"https://miro.medium.com/max/1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5b825-09be-4bea-99a5-8750ab6fb266",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-flaticons-lineal-color-flat-icons/344/external-case-study-social-media-agency-flaticons-lineal-color-flat-icons-2.png\" width=50> <font size=5> Case Study:</font><br>\n",
    "Here, we want to build a predictive model for stock market using gradient descent. We use General Mills stock market (Ticker: GIS). The data is downloaded from Yahoo Finanace and stored in the file \"GIS.csv\". Load the data and build an AR model for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c797f-97de-4233-8fda-1c34b28e4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting General Mills Stocks using an AR model\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "gis = pd.read_csv(\"GIS.csv\") # This data is downloaded from Yahoo finance.\n",
    "gis = gis[[\"Date\", \"Close\"]] # Keeping closing value and date.\n",
    "fig = plt.figure(figsize=(18,9))\n",
    "x_ticks = gis[\"Date\"][range(1,gis.shape[0], 8)]\n",
    "ax = sns.lineplot(data=gis, x=\"Date\", y=\"Close\")\n",
    "_=plt.xticks(x_ticks, rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99de98-4641-46d4-b8be-52f1d0ebd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags(df, p):\n",
    "    close = df[\"Close\"].to_list()\n",
    "    df[\"const\"] = 1.\n",
    "    for lag in range(1,p):\n",
    "        col_name = \"lag_\" + str(lag)\n",
    "        df[col_name] = np.array([0] * lag + close[:-lag])\n",
    "       \n",
    "    return df.iloc[p:]\n",
    "\n",
    "p = 4\n",
    "gis_cpy = gis.copy()\n",
    "gis_cpy = add_lags(gis_cpy, p)\n",
    "#fig = plt.figure(figsize=(18,9))\n",
    "#ax = sns.lineplot(data=gis_cpy, x=\"Date\", y=\"Close\")\n",
    "#for i in range(1,p ):\n",
    "#    sns.lineplot(data=gis_cpy, x=\"Date\", y=\"lag_\" + str(i))\n",
    "#_=plt.xticks(x_ticks, rotation=30)\n",
    "y = tf.constant(gis_cpy[\"Close\"].to_numpy(), dtype=tf.double)\n",
    "x_cols = [\"const\"] + [c for c in gis_cpy.columns if c.startswith(\"lag\")]\n",
    "x = tf.constant(gis_cpy[x_cols].to_numpy(), dtype=tf.double)\n",
    "t = gis_cpy[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4ebf1-fc60-4c10-a317-02f4f94c54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters -> y[n] = a_0 + a_1 * y[n-1] + a_2 * y[n-2] + ...+ a_p * y[n-p]\n",
    "a = [tf.Variable(np.random.rand(1)[0], dtype=tf.double, name=\"a_\" + str(i)) for i in range(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763d3f9-8d84-4253-bb0a-5b10a45e689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the SSE loss for finding optimal a_i's\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.003, rho=0.9,momentum=0.1)\n",
    "for i in range(1000):\n",
    "    loss = lambda: tf.reduce_sum((tf.linalg.matvec(x, a) - y)**2)\n",
    "    step_count = opt.minimize(loss, a)\n",
    "print(\"Loss: \", loss())\n",
    "print(\"Coefficients: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2baba40-7bd0-48d5-a872-45398254fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tf.linalg.matvec(x, a).numpy()\n",
    "fig = plt.figure(figsize=(18,9))\n",
    "sns.lineplot(x=t, y=y.numpy(), label=\"Actual\")\n",
    "sns.lineplot(x=t, y=y_hat, label=\"Forecast\")\n",
    "plt.setp(plt.gca().get_legend().get_texts(), fontsize='22')\n",
    "_=plt.xticks(x_ticks, rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aac06a-9234-4931-8fd3-5f386807ce88",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Adam</h2>\n",
    "Adam is basically mixing momentum with RMSprop. It has been widely used in different application and is one the most efficient versions of gradient descent. Adam applies bias correction to momentum and decay. As t goes higher the weight of these terms decay.<br>\n",
    "<img src=\"images/adam.png\" width=250><br>\n",
    "In tensorflow you can use <code>tf.keras.optimizers.Adam(learning_rate, beta_1, beta_2)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593ed71-7e50-48db-a174-aa89f7bf2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example let's solve the stock market prediction using Adam\n",
    "a = [tf.Variable(np.random.rand(1)[0], dtype=tf.double, name=\"a_\" + str(i)) for i in range(p)]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1,beta_1=0.9, beta_2=0.999)\n",
    "for i in range(800):\n",
    "    loss = lambda: tf.reduce_sum((tf.linalg.matvec(x, a) - y)**2)\n",
    "    step_count = opt.minimize(loss, a)\n",
    "print(\"Loss: \", loss())\n",
    "print(\"Coefficients: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b444e5c-0061-42ce-8238-7b3b5159c478",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=50>__Tip__: Tensorflow has two variants of Adam. They are called Adamax and NAdam. Adamax is adam, it just uses max norm. NAdam uses Nesterov instead of momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20624c-6dee-43fb-8f36-406a4120c989",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=50>__Tip__: Tensorflow has another variant of GD called FTRL (Follow The Regularized Leader). FTRL is developed at Google for click-through rate prediction in the early 2010s. It is most suitable for shallow models with large and sparse feature spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725df6a6-d8a3-4919-b52c-ddcdd4e7e81c",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-flaticons-lineal-color-flat-icons/344/external-coffee-cup-bakery-flaticons-lineal-color-flat-icons.png\" alt=\"Takehome\" width=50><font size=4>Burg Method:</font><br>\n",
    "In the Market stock case study, we minimize the forward prediciton error. which means error of predicting x[n] using x[n-1] ... x[n-p]. It is also possible to define a backward error which is predicting x[n-p] using x[n-p+1] ...x[n]. In burg method for AR both forward and backward errors are minimized. Although, both forward and burg objectives have closed form solution, here we try to solve them using gradient descent. Write a program which uses Adam to minimize burg objective for the case study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186a0f3-ab60-42db-aad8-eef8374da092",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-soft-fill-juicy-fish/344/external-maths-school-soft-fill-soft-fill-juicy-fish.png\" alt=\"Math Tip\" width=50><font size=4> Gradient Clipping:</font><br>\n",
    "One of issues with gradient descent is <em>exploding gradient</em> which is the case that gradient become too large. This raise certain issues such as instability and divergence. Also it can cause running out the precision of double numbers. A treatment for this issue is <em>gradient clipping </em> which bounds the magnitude of gradient and preserving its direction. There are different ways of clipping. Check Tensorflow docs for more details. Here, Let's check the concept using an exmple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583aa80-5eea-4b25-a857-22d3557b642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tf.Variable(np.random.rand(1)[0], dtype=tf.double, name=\"a_\" + str(i)) for i in range(p)]\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.003, rho=0.9,momentum=0.1)\n",
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_sum((tf.linalg.matvec(x, a) - y)**2)\n",
    "    grads = tape.gradient(loss, a)\n",
    "    grads = [tf.clip_by_norm(g, 100.) for g in grads]\n",
    "    opt.apply_gradients(zip(grads, a))\n",
    "    \n",
    "print(\"Loss: \", loss)\n",
    "print(\"Coefficients: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca9e04-4d7d-4ded-81be-4b348420c8e4",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/external-flaticons-lineal-color-flat-icons/344/external-coffee-cup-bakery-flaticons-lineal-color-flat-icons.png\" alt=\"Takehome\" width=50><font size=5> Constrained Optimization Takehome:</font>\n",
    "What we have discussed so far was about unconstrained optimization. Tensorflow originally built for training DNNs and constrained optimization is not very relevant. However, there is an extension called TensorFlow Constrained Optimization (TFCO). <br>\n",
    "For installing TFCO use: <code>pip install tensorflow-constrained-optimization</code>\n",
    "We don't cover the TFCO here as it is not very mature. But, we leave it to you as a take home. The notebook `Takehome_TFCO_Oscillation_compas` contains an example using TFCO, go and check it on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e2aa8-29e1-49f0-aec9-1005a07abd26",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/color/344/light.png\" width=50><font size=5> Tips for Performance:</font><br>\n",
    "Tensorflow is a high performance distributed computing library. It can run the code on multiple CPUs, GPUs and TPUs. Check tensorflow docs for <code>tf.distribute.Strategy</code>. It allows you to define your distributed computing strategy. <br>\n",
    "Also, so far we used the eager API which runs tensors and operators like Python. Tensorflow supports a lazy evaluation API called graphs, which makes exporting the problem to other platforms easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
