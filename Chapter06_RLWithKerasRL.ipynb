{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bb8895-6d5f-48e7-a94b-101ccbb8a634",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1> Reinforcement Learning Using KerasRL</h1>\n",
    "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem. The picture below shows RL settings, where the agent interacts with the environment. The agent can take an actions from set of actions (for example move up in a game). The action take the agent/enviroment to the same or a new state. Then, the agent is rewarded for that.  \n",
    "<img src=\"images/rl.png\" width=500><br>\n",
    "RL is studied in different fields with different names like optimal control theory, game theory, Operation research. There are a variety of approaches to solve the RL problems.  <br><br>\n",
    "There are some packages for RL in python. Here we focus on Keras-RL. <br>\n",
    "For installing keras-rl you can use: <code>pip install keras-rl</code><br>\n",
    "For using Keras-RL you might need to install openmpi on your machine.<br>\n",
    "\n",
    "<br>Also for simulating environments, we are going to use gym. Gym has some classic RL problems and provide animation representation of the environment. \n",
    "<br>For installing gym on your machine you should use:<code>conda install -c conda-forge gym</code>\n",
    "Let's explore gym first before going further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5f5e6-e655-4783-a45f-2434f635cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on mac BigSur has issues with gym and raises many warnings. igonore them for now.\n",
    "# This cell pop ups another window which show the animated environment\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc83da4-9375-44e6-8ede-6dcde60f6e40",
   "metadata": {},
   "source": [
    "The step function interacts with the environment and returns four values as follow:\n",
    "<ul>\n",
    "  <li><code class=\"highlighter-rouge\">observation</code> (<strong>object</strong>): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.</li>\n",
    "  <li><code class=\"highlighter-rouge\">reward</code> (<strong>float</strong>): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.</li>\n",
    "  <li><code class=\"highlighter-rouge\">done</code> (<strong>boolean</strong>): whether it’s time to <code class=\"highlighter-rouge\">reset</code> the environment again. Most (but not all) tasks are divided up into well-defined episodes, and <code class=\"highlighter-rouge\">done</code> being <code class=\"highlighter-rouge\">True</code> indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)</li>\n",
    "  <li><code class=\"highlighter-rouge\">info</code> (<strong>dict</strong>): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7652f6-3eb7-42a0-b743-1215b56563e6",
   "metadata": {},
   "source": [
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an action, and the environment returns an observation and a reward. The process gets started by calling reset(), which returns an initial observation. So a more proper way of writing the previous code would be to respect the done flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9960ee-e1e4-455b-b6c2-cc22e1d337d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better way of implementing agent-env loop\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(10):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc156f7-d86a-4fa5-9d52-6638dea5bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d400d-4d1a-43ca-9dd9-41f068c166ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287272bd-45b1-4e9c-a034-11a0efb37180",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render(); Adding this line would show the attempts\n",
    "            # of the agent in a pop up window.\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
