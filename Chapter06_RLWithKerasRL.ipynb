{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bb8895-6d5f-48e7-a94b-101ccbb8a634",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1> Reinforcement Learning Using KerasRL</h1>\n",
    "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem. The picture below shows RL settings, where the agent interacts with the environment. The agent can take an actions from set of actions (for example move up in a game). The action take the agent/enviroment to the same or a new state. Then, the agent is rewarded for that.  \n",
    "<img src=\"images/rl.png\" width=500><br>\n",
    "RL is studied in different fields with different names like optimal control theory, game theory, Operation research. There are a variety of approaches to solve the RL problems.  <br><br>\n",
    "There are some packages for RL in python. Here we focus on Keras-RL. <br>\n",
    "For installing keras-rl you can use: <code>pip install keras-rl</code><br>\n",
    "For using Keras-RL you might need to install openmpi on your machine.<br>\n",
    "\n",
    "<br>Also for simulating environments, we are going to use gym. Gym has some classic RL problems and provide animation representation of the environment. \n",
    "<br>For installing gym on your machine you should use:<code>conda install -c conda-forge gym</code>\n",
    "Let's explore gym first before going further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5f5e6-e655-4783-a45f-2434f635cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on mac BigSur has issues with gym and raises many warnings. igonore them for now.\n",
    "# This cell pop ups another window which show the animated environment\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc83da4-9375-44e6-8ede-6dcde60f6e40",
   "metadata": {},
   "source": [
    "The step function interacts with the environment and returns four values as follow:\n",
    "<ul>\n",
    "  <li><code class=\"highlighter-rouge\">observation</code> (<strong>object</strong>): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.</li>\n",
    "  <li><code class=\"highlighter-rouge\">reward</code> (<strong>float</strong>): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.</li>\n",
    "  <li><code class=\"highlighter-rouge\">done</code> (<strong>boolean</strong>): whether it’s time to <code class=\"highlighter-rouge\">reset</code> the environment again. Most (but not all) tasks are divided up into well-defined episodes, and <code class=\"highlighter-rouge\">done</code> being <code class=\"highlighter-rouge\">True</code> indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)</li>\n",
    "  <li><code class=\"highlighter-rouge\">info</code> (<strong>dict</strong>): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7652f6-3eb7-42a0-b743-1215b56563e6",
   "metadata": {},
   "source": [
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an action, and the environment returns an observation and a reward. The process gets started by calling reset(), which returns an initial observation. So a more proper way of writing the previous code would be to respect the done flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9960ee-e1e4-455b-b6c2-cc22e1d337d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better way of implementing agent-env loop\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(10):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e819d3f-c346-4370-8f12-839b20bd5f52",
   "metadata": {},
   "source": [
    "<h2> Cart Pole V0 System</h2>\n",
    "In the CartPole-v0 environment, a pole is attached to a cart moving along a frictionless track. The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. A reward of +1 is given for every time step the pole remains upright. An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.\n",
    "<br>The problem is considered \"solved\" when the average total reward for the episode reaches 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa0958-5b18-4194-91b6-b2a430fab7d4",
   "metadata": {},
   "source": [
    "<h1>Actor - Critic Learning</h1>\n",
    "<h2> Introduction</h2>\n",
    "If you’re learning to play Go, one of the best ways to improve is to get a stronger player to review your games. Sometimes the most useful feedback just points out where you won or lost the game. The reviewer might give comments like, “You were already far behind by move 30” or “At move 110, you had a winning position, but your opponent turned it around by move 130.”\n",
    "\n",
    "Why is this feedback helpful? You may not have time to scrutinize all 300 moves in a game, but you can focus your full attention on a 10- or 20-move sequence. The reviewer lets you know which parts of the game are important.\n",
    "\n",
    "Reinforcement-learning researchers apply this principle in actor-critic learning, which is a combination of policy learning  and value learning . The policy function plays the role of the actor: it picks what moves to play. The value function is the critic: it tracks whether the agent is ahead or behind in the course of the game. That feedback guides the training process, in the same way that a game review can guide your own study.\n",
    "<h2>Method</h2>\n",
    "Actor-Critic methods are temporal difference (TD) learning methods that represent the policy function independent of the value function.\n",
    "\n",
    "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state. A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
    "\n",
    "In the Actor-Critic method, the policy is referred to as the actor that proposes a set of possible actions given a state, and the estimated value function is referred to as the critic, which evaluates actions taken by the actor based on the given policy. In this tutorial, both the Actor and Critic will be represented using one neural network with two outputs.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc156f7-d86a-4fa5-9d52-6638dea5bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec74296-30c8-4a5a-b784-c246ceb66b90",
   "metadata": {},
   "source": [
    "<h2>Model</h2>\n",
    "The Actor and Critic will be modeled using one neural network that generates the action probabilities and critic value respectively. \n",
    "\n",
    "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value V, which models the state-dependent value function. The goal is to train a model that chooses actions based on a policy π that maximizes expected return.\n",
    "\n",
    "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d400d-4d1a-43ca-9dd9-41f068c166ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d928d-cb9a-4a53-90eb-46b8dfb7da6b",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "To train the agent, you will follow these steps:\n",
    "\n",
    "Run the agent on the environment to collect training data per episode.\n",
    "<ol>\n",
    "<li>Run the agent on the environment to collect training data per episode.</li>\n",
    "<li>Compute expected return at each time step.</li>\n",
    "<li>Compute the loss for the combined actor-critic model.</li>\n",
    "<li>Compute gradients and update network parameters.</li>\n",
    "<li>Repeat 1-4 until either success criterion or max episodes has been reached.</li>\n",
    "</ol>\n",
    "<h3>Collecting Data</h3>\n",
    "As in supervised learning, in order to train the actor-critic model, you need to have training data. However, in order to collect such data, the model would need to be \"run\" in the environment.\n",
    "\n",
    "Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
    "\n",
    "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated.\n",
    "\n",
    "\n",
    "<h2> Comupting the expected return</h2>\n",
    "The sequence of rewards for each timestep t, {r_t} for t= 1,...,T  collected during one episode is converted into a sequence of expected returns {G_t} for t=1,..., T  in which the sum of rewards is taken from the current timestep t to T and each reward is multiplied with an exponentially decaying discount factor :\n",
    "\n",
    "<img src=\"images/reward.png\" width=200>\n",
    "Since  γ is between 0 and 1, rewards further out from the current timestep are given less weight.\n",
    "\n",
    "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
    "\n",
    "<h2> The Critic Loss</h2>\n",
    "Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:\n",
    "<br>Loss = L_Actor + L_Critic<br>\n",
    "The actor loss is based on policy gradients with the critic as a state dependent baseline and computed with single-sample (per-episode) estimates.\n",
    "<img src=\"images/l_actor.png\" width =350>\n",
    "here:<br>\n",
    "\n",
    "T : the number of timesteps per episode, which can vary per episode<br>\n",
    "s_t: the state at timestep t<br>\n",
    "a_t: chosen action at timestep t given state s<br>\n",
    "π_Θ: is the policy (actor) parameterized by Θ<br>\n",
    "V_Θ^π: is the value function (critic) also parameterized by π<br>\n",
    "G: G_t the expected return for a given state, action pair at timestep t<br>\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<h2> Advantage</h2>\n",
    "the G-V term in our L_actor formulation is called the advantage, which indicates how much better an action is given a particular state over a random action selected according to the policy π for that state.\n",
    "\n",
    "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic V as a baseline is that it trained to be as close as possible to G, leading to a lower variance.\n",
    "\n",
    "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
    "\n",
    "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective J. With the critic, it may turn out that there's no advantage (G-V) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
    "\n",
    "<h2>Critic Loss</h2>\n",
    "Training V to be as close possible to G can be set up as a regression problem with the following loss function:<br>\n",
    "<img src=\"images/critic_loss.png\" width=150>\n",
    "where Lƍ is the Huber loss, which is less sensitive to outliers in data than squared-error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287272bd-45b1-4e9c-a034-11a0efb37180",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render(); Adding this line would show the attempts\n",
    "            # of the agent in a pop up window.\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ca90c-d28a-4cda-8272-0ffa8a0a1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the trained model.\n",
    "for i_episode in range(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):\n",
    "        env.render()\n",
    "        print(state)\n",
    "        # action = env.action_space.sample()\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "        action_probs, critic_value = model(state)\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
